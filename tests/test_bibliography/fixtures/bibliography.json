[
  {"id":"bengioAdvancesOptimizingRecurrent2012","abstract":"After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modelling sequences,their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.","accessed":{"date-parts":[["2023",7,22]]},"author":[{"family":"Bengio","given":"Yoshua"},{"family":"Boulanger-Lewandowski","given":"Nicolas"},{"family":"Pascanu","given":"Razvan"}],"citation-key":"bengioAdvancesOptimizingRecurrent2012","issued":{"date-parts":[["2012",12,13]]},"number":"arXiv:1212.0901","publisher":"arXiv","source":"arXiv.org","title":"Advances in Optimizing Recurrent Networks","type":"article","URL":"http://arxiv.org/abs/1212.0901"},
  {"id":"dettmers8bitOptimizersBlockwise2022","abstract":"Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization compared to plain stochastic gradient descent but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14 machine translation, MoCo v2 contrastive ImageNet pretraining+finetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-source our 8-bit optimizers as a drop-in replacement that only requires a two-line code change.","accessed":{"date-parts":[["2023",7,22]]},"author":[{"family":"Dettmers","given":"Tim"},{"family":"Lewis","given":"Mike"},{"family":"Shleifer","given":"Sam"},{"family":"Zettlemoyer","given":"Luke"}],"citation-key":"dettmers8bitOptimizersBlockwise2022","DOI":"10.48550/arXiv.2110.02861","issued":{"date-parts":[["2022",6,20]]},"number":"arXiv:2110.02861","publisher":"arXiv","source":"arXiv.org","title":"8-bit Optimizers via Block-wise Quantization","type":"article","URL":"http://arxiv.org/abs/2110.02861"},
  {"id":"grosseKroneckerfactoredApproximateFisher2016","abstract":"Second-order optimization methods such as natural gradient descent have the potential to speed up training of neural networks by correcting for the curvature of the loss function. Unfortunately, the exact natural gradient is impractical to compute for large models, and most approximations either require an expensive iterative procedure or make crude approximations to the curvature. We present Kronecker Factors for Convolution (KFC), a tractable approximation to the Fisher matrix for convolutional networks based on a structured probabilistic model for the distribution over backpropagated derivatives. Similarly to the recently proposed Kronecker-Factored Approximate Curvature (K-FAC), each block of the approximate Fisher matrix decomposes as the Kronecker product of small matrices, allowing for efficient inversion. KFC captures important curvature information while still yielding comparably efficient updates to stochastic gradient descent (SGD). We show that the updates are invariant to commonly used reparameterizations, such as centering of the activations. In our experiments, approximate natural gradient descent with KFC was able to train convolutional networks several times faster than carefully tuned SGD. Furthermore, it was able to train the networks in 10-20 times fewer iterations than SGD, suggesting its potential applicability in a distributed setting.","accessed":{"date-parts":[["2023",4,14]]},"author":[{"family":"Grosse","given":"Roger"},{"family":"Martens","given":"James"}],"citation-key":"grosseKroneckerfactoredApproximateFisher2016","DOI":"10.48550/arXiv.1602.01407","issued":{"date-parts":[["2016",5,23]]},"number":"arXiv:1602.01407","publisher":"arXiv","source":"arXiv.org","title":"A Kronecker-factored approximate Fisher matrix for convolution layers","type":"article","URL":"http://arxiv.org/abs/1602.01407"},
  {"id":"guptaShampooPreconditionedStochastic2018","abstract":"Preconditioned gradient methods are among the most general and powerful tools in optimization. However, preconditioning requires storing and manipulating prohibitively large matrices. We describe and analyze a new structure-aware preconditioning algorithm, called Shampoo, for stochastic optimization over tensor spaces. Shampoo maintains a set of preconditioning matrices, each of which operates on a single dimension, contracting over the remaining dimensions. We establish convergence guarantees in the stochastic convex setting, the proof of which builds upon matrix trace inequalities. Our experiments with state-of-the-art deep learning models show that Shampoo is capable of converging considerably faster than commonly used optimizers. Although it involves a more complex update rule, Shampoo's runtime per step is comparable to that of simple gradient methods such as SGD, AdaGrad, and Adam.","accessed":{"date-parts":[["2023",4,14]]},"author":[{"family":"Gupta","given":"Vineet"},{"family":"Koren","given":"Tomer"},{"family":"Singer","given":"Yoram"}],"citation-key":"guptaShampooPreconditionedStochastic2018","DOI":"10.48550/arXiv.1802.09568","issued":{"date-parts":[["2018",3,1]]},"number":"arXiv:1802.09568","publisher":"arXiv","source":"arXiv.org","title":"Shampoo: Preconditioned Stochastic Tensor Optimization","title-short":"Shampoo","type":"article","URL":"http://arxiv.org/abs/1802.09568"},
  {"id":"kingmaAdamMethodStochastic2017","abstract":"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.","accessed":{"date-parts":[["2023",7,14]]},"author":[{"family":"Kingma","given":"Diederik P."},{"family":"Ba","given":"Jimmy"}],"citation-key":"kingmaAdamMethodStochastic2017","DOI":"10.48550/arXiv.1412.6980","issued":{"date-parts":[["2017",1,29]]},"number":"arXiv:1412.6980","publisher":"arXiv","source":"arXiv.org","title":"Adam: A Method for Stochastic Optimization","title-short":"Adam","type":"article","URL":"http://arxiv.org/abs/1412.6980"},
  {"id":"loshchilovDecoupledWeightDecay2019","abstract":"L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW","accessed":{"date-parts":[["2023",4,4]]},"author":[{"family":"Loshchilov","given":"Ilya"},{"family":"Hutter","given":"Frank"}],"citation-key":"loshchilovDecoupledWeightDecay2019","DOI":"10.48550/arXiv.1711.05101","issued":{"date-parts":[["2019",1,4]]},"number":"arXiv:1711.05101","publisher":"arXiv","source":"arXiv.org","title":"Decoupled Weight Decay Regularization","type":"article","URL":"http://arxiv.org/abs/1711.05101"},
  {"id":"pauloskiDeepNeuralNetwork2022","abstract":"Scaling deep neural network training to more processors and larger batch sizes is key to reducing end-to-end training time; yet, maintaining comparable convergence and hardware utilization at larger scales is challenging. Increases in training scales have enabled natural gradient optimization methods as a reasonable alternative to stochastic gradient descent and variants thereof. Kronecker-factored Approximate Curvature (K-FAC), a natural gradient method, preconditions gradients with an efficient approximation of the Fisher Information Matrix to improve per-iteration progress when optimizing an objective function. Here we propose a scalable K-FAC algorithm and investigate K-FAC’s applicability in large-scale deep neural network training. Specifically, we explore layer-wise distribution strategies, inverse-free second-order gradient evaluation, and dynamic K-FAC update decoupling, with the goal of preserving convergence while minimizing training time. We evaluate the convergence and scaling properties of our K-FAC gradient preconditioner, for image classification, object detection, and language modeling applications. In all applications, our implementation converges to baseline performance targets in 9–25% less time than the standard first-order optimizers on GPU clusters across a variety of scales.","author":[{"family":"Pauloski","given":"J. Gregory"},{"family":"Huang","given":"Lei"},{"family":"Xu","given":"Weijia"},{"family":"Chard","given":"Kyle"},{"family":"Foster","given":"Ian T."},{"family":"Zhang","given":"Zhao"}],"citation-key":"pauloskiDeepNeuralNetwork2022","container-title":"IEEE Transactions on Parallel and Distributed Systems","DOI":"10.1109/TPDS.2022.3161187","ISSN":"1558-2183","issue":"12","issued":{"date-parts":[["2022",12]]},"page":"3616-3627","source":"IEEE Xplore","title":"Deep Neural Network Training With Distributed K-FAC","type":"article-journal","volume":"33"},
  {"id":"shazeerAdafactorAdaptiveLearning2018","abstract":"In several recently proposed stochastic optimization methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves.","accessed":{"date-parts":[["2023",4,13]]},"author":[{"family":"Shazeer","given":"Noam"},{"family":"Stern","given":"Mitchell"}],"citation-key":"shazeerAdafactorAdaptiveLearning2018","DOI":"10.48550/arXiv.1804.04235","issued":{"date-parts":[["2018",4,11]]},"number":"arXiv:1804.04235","publisher":"arXiv","source":"arXiv.org","title":"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost","title-short":"Adafactor","type":"article","URL":"http://arxiv.org/abs/1804.04235"},
  {"id":"youLargeBatchOptimization2020","abstract":"Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available at https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py","accessed":{"date-parts":[["2023",4,14]]},"author":[{"family":"You","given":"Yang"},{"family":"Li","given":"Jing"},{"family":"Reddi","given":"Sashank"},{"family":"Hseu","given":"Jonathan"},{"family":"Kumar","given":"Sanjiv"},{"family":"Bhojanapalli","given":"Srinadh"},{"family":"Song","given":"Xiaodan"},{"family":"Demmel","given":"James"},{"family":"Keutzer","given":"Kurt"},{"family":"Hsieh","given":"Cho-Jui"}],"citation-key":"youLargeBatchOptimization2020","DOI":"10.48550/arXiv.1904.00962","issued":{"date-parts":[["2020",1,3]]},"number":"arXiv:1904.00962","publisher":"arXiv","source":"arXiv.org","title":"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes","title-short":"Large Batch Optimization for Deep Learning","type":"article","URL":"http://arxiv.org/abs/1904.00962"},
  {"id":"zeilerADADELTAAdaptiveLearning2012","abstract":"We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.","accessed":{"date-parts":[["2023",7,22]]},"author":[{"family":"Zeiler","given":"Matthew D."}],"citation-key":"zeilerADADELTAAdaptiveLearning2012","issued":{"date-parts":[["2012",12,22]]},"number":"arXiv:1212.5701","publisher":"arXiv","source":"arXiv.org","title":"ADADELTA: An Adaptive Learning Rate Method","title-short":"ADADELTA","type":"article","URL":"http://arxiv.org/abs/1212.5701"},
  {"id":"bottouOptimizationMethodsLargeScale2018","abstract":"The question of how to incorporate curvature information into stochastic approximation methods is challenging. The direct application of classical quasi-Newton updating techniques for deterministic optimization leads to noisy curvature estimates that have harmful effects on the robustness of the iteration.  In this paper, we propose a stochastic quasi-Newton method that is efficient, robust, and scalable. It employs the classical BFGS update formula in its limited memory form, and is based on the observation that it is beneficial to collect curvature information pointwise, and at  spaced  intervals. One way to do this is through  (subsampled) Hessian-vector products. This technique differs from the classical approach that would compute differences of gradients at every iteration, and where controlling the quality of the curvature estimates can be difficult.  We present numerical results on problems arising in machine learning that suggest that the proposed method shows much promise.","accessed":{"date-parts":[["2023",3,30]]},"author":[{"family":"Bottou","given":"Léon"},{"family":"Curtis","given":"Frank E."},{"family":"Nocedal","given":"Jorge"}],"citation-key":"bottouOptimizationMethodsLargeScale2018","container-title":"SIAM Review","container-title-short":"SIAM Rev.","DOI":"10.1137/16M1080173","ISSN":"0036-1445","issue":"2","issued":{"date-parts":[["2018",1]]},"page":"223-311","publisher":"Society for Industrial and Applied Mathematics","source":"epubs.siam.org (Atypon)","title":"Optimization Methods for Large-Scale Machine Learning","type":"article-journal","URL":"https://epubs.siam.org/doi/10.1137/16M1080173","volume":"60"},
  {"id":"boydConvexOptimization2004","author":[{"family":"Boyd","given":"Stephen P."},{"family":"Vandenberghe","given":"Lieven"}],"call-number":"QA402.5 .B69 2004","citation-key":"boydConvexOptimization2004","event-place":"Cambridge, UK ; New York","ISBN":"978-0-521-83378-3","issued":{"date-parts":[["2004"]]},"number-of-pages":"716","publisher":"Cambridge University Press","publisher-place":"Cambridge, UK ; New York","source":"Library of Congress ISBN","title":"Convex optimization","type":"book"},
  {"id":"ningqianMomentumTermGradient1999","abstract":"A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.","accessed":{"date-parts":[["2023",3,30]]},"author":[{"literal":"Ning Qian"}],"citation-key":"ningqianMomentumTermGradient1999","container-title":"Neural Networks","container-title-short":"Neural Networks","DOI":"10.1016/S0893-6080(98)00116-6","ISSN":"0893-6080","issue":"1","issued":{"date-parts":[["1999",1,1]]},"language":"en","page":"145-151","source":"ScienceDirect","title":"On the momentum term in gradient descent learning algorithms","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0893608098001166","volume":"12"},
  {"id":"suttonProblemsWithBackpropagation1986","author":[{"family":"Sutton","given":"Richard S."}],"citation-key":"suttonProblemsWithBackpropagation1986","container-title":"Proceedings of the eighth annual conference of the cognitive science society","issued":{"date-parts":[["1986"]]},"publisher":"Hillsdale, NJ: Erlbaum","title":"Two problems with backpropagation and other steepest-descent learning procedures for networks","type":"paper-conference"}
]
